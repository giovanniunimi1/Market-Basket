{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpJhNGg3To/MUI+jqJhqSw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giovanniunimi1/Market-Basket/blob/main/Market_Basket.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO3-iKwiJsXG"
      },
      "outputs": [],
      "source": [
        " #DOWNLOAD SPARK AND SPARK DEPENDENCIES (JDK AND FINDSPARK)\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n",
        "\n",
        "! pip install -q kaggle\n",
        "#set kaggle for dataset (remove # for setting kaggle api for downloading dataset)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"giovannibuscemi\"\n",
        "os.environ['KAGGLE_KEY'] = \"Bsc120199.gg\"\n",
        "\n",
        "\n",
        "!pip install pyngrok\n",
        "import getpass\n",
        "from pyngrok import ngrok, conf\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "import findspark\n",
        "import zipfile\n",
        "! kaggle datasets download -d gsimonx37/letterboxd\n",
        "#! unzip letterboxd.zip -d /letterboxd\n",
        "extract_to_path = \"/letterboxd\"\n",
        "with zipfile.ZipFile(\"letterboxd.zip\", 'r') as zip_ref:\n",
        "    for file in zip_ref.namelist():\n",
        "        if file.endswith('.csv'):\n",
        "            zip_ref.extract(file, extract_to_path)\n",
        "\n",
        "def map_names(row):\n",
        "  return(row.id,names_map[row.name])\n",
        "\n",
        "def get_candidate_set(frequent_itemset,k):\n",
        "  candidate_set = set()\n",
        "  n=len(frequent_itemset)\n",
        "  for i in range(n):\n",
        "    for j in range(i+1,n):\n",
        "      candidate =tuple(sorted([frequent_itemset[i],frequent_itemset[j]]))\n",
        "      candidate_set.add(candidate)\n",
        "  return list(candidate_set)\n",
        "\n",
        "\n",
        "def apriori(iterator):\n",
        "  baskets = list(iterator)\n",
        "\n",
        "  table_counts = defaultdict(int)\n",
        "\n",
        "  for basket in baskets:\n",
        "    for item in basket:\n",
        "      table_counts[item] += 1\n",
        "\n",
        "  frequent_items = [item for item, count in table_counts.items() if count >= support]\n",
        "\n",
        "  candidate_set = get_candidate_set(frequent_items,2)\n",
        "\n",
        "  candidate_counts = defaultdict(int)\n",
        "  for basket in baskets:\n",
        "    for candidate in candidate_set:\n",
        "       if all(element in basket for element in candidate):\n",
        "           candidate_counts[candidate] += 1\n",
        "  #filter infrequent itemset\n",
        "  frequent_itemset =[candidate for candidate,count in candidate_counts.items() if count >= support]\n",
        "  return frequent_itemset\n",
        "\n",
        "def count_itemset(iterator,candidate_set):\n",
        "  baskets = list(iterator)\n",
        "  local_count = []\n",
        "\n",
        "  for candidate in candidate_set:\n",
        "    count = 0\n",
        "    for basket in baskets:\n",
        "      if all(element in basket for element in candidate):\n",
        "        count +=1\n",
        "    local_count.append((candidate,count))\n",
        "  return local_count\n",
        "\n",
        "def count_single_items(iterator,single_items):\n",
        "  baskets = list(iterator)\n",
        "  local_count = defaultdict(int)\n",
        "\n",
        "  for basket in baskets:\n",
        "    for item in single_items:\n",
        "      if item in basket:\n",
        "        local_count[item]+=1\n",
        "  return local_count.items()\n",
        "\n",
        "def calculate_confidence(frequent_itemset_counts,single_item_counts):\n",
        "  confidence = {}\n",
        "  for itemset,count in frequent_itemset_counts:\n",
        "      confidence[(int_map[itemset[0]],int_map[itemset[1]])] = count / single_item_counts[itemset[0]]\n",
        "      confidence[(int_map[itemset[1]],int_map[itemset[0]])] = count / single_item_counts[itemset[1]]\n",
        "\n",
        "  return confidence\n",
        "#GLOBAL PARAMETER : FRACTION OF THE DATASET, SUPPORT\n",
        "fraction = 0.014\n",
        "support = 4\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count,collect_list\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").config('spark.ui.port', '4050').appName(\"A-priori\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "actors = spark.read.csv(\"/letterboxd/actors.csv\", header=True, inferSchema=True)\n",
        "\n",
        "\n",
        "#dict for names\n",
        "names = actors.select(\"name\").distinct().rdd.map(lambda row : row[0]).collect()\n",
        "names_map = {name : i for i,name in enumerate(names)}\n",
        "int_map = {idx :name for name,idx in names_map.items()}\n",
        "\n",
        "\n",
        "all_baskets = actors.rdd.map(map_names).groupByKey().map(lambda x: tuple(x[1]))\n",
        "baskets = all_baskets.sample(False,fraction)\n",
        "numPartitions = baskets.getNumPartitions()\n",
        "\n",
        "local_support = support/numPartitions +1\n",
        "\n",
        "#MapReduce phase 1\n",
        "candidate_itemset = baskets.mapPartitions(lambda partition: apriori(partition)).collect()\n",
        "#eliminate duplicates\n",
        "candidate_itemset = list(set(candidate_itemset))\n",
        "\n",
        "#MapReduce phase 2\n",
        "frequent_itemset = baskets.mapPartitions(lambda partition: count_itemset(partition,candidate_itemset)).reduceByKey(lambda x,y: x+y).filter(lambda row : row[1]>=local_support).collect()\n",
        "#Operation for calculate Confidence\n",
        "single_items = set()\n",
        "for itemset,_ in frequent_itemset :\n",
        "  for item in itemset:\n",
        "    single_items.add(item)\n",
        "\n",
        "single_item_counts = baskets.mapPartitions(lambda partition : count_single_items(partition,list(single_items))).reduceByKey(lambda x,y : x+y).collectAsMap()\n",
        "print(\"Frequent itemset :\")\n",
        "print(frequent_itemset)\n",
        "\n",
        "confidence = calculate_confidence(frequent_itemset,single_item_counts)\n",
        "print(\"Confidence for frequent itemset\")\n",
        "print(confidence)\n",
        "spark.stop()"
      ]
    }
  ]
}